{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96840170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "# define the submission/grader object for this exercise\n",
    "grader = utils.Grader()\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ead3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('Data', 'ex4data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Number of training examples\n",
    "m = y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78afc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat(os.path.join('Data', 'ex4weights.mat'))\n",
    "\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a5535",
   "metadata": {},
   "source": [
    "# DATA SUMMARY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed523892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 5000\n",
      "Size of X = (5000, 400)\n",
      "Hidden layer size = 25\n",
      "The size of Theta1 = (25, 401)\n",
      "The size of Theta2 = (10, 26)\n",
      "Size of encoded y = (5000, 10)\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] ~~ y = 4\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] ~~ y = 1\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] ~~ y = 5\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] ~~ y = 3\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] ~~ y = 7\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] ~~ y = 8\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] ~~ y = 5\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] ~~ y = 3\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] ~~ y = 2\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] ~~ y = 3\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples =', m)\n",
    "print('Size of X =', X.shape)\n",
    "print('Hidden layer size =', hidden_layer_size)\n",
    "print('The size of Theta1 =', Theta1.shape)\n",
    "print('The size of Theta2 =', Theta2.shape)\n",
    "\n",
    "temp_y = np.zeros((m, 10))\n",
    "for i, num in enumerate(y):\n",
    "    temp_y[i][num] = 1\n",
    "print('Size of encoded y =', temp_y.shape)\n",
    "    \n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "for i in range (10):\n",
    "    print (temp_y[rand_indices[i]], '~~ y =', y[rand_indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "958ad95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    g = np.zeros(z.shape)\n",
    "    g = utils.sigmoid(z) * (1 - utils.sigmoid(z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fa73d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of delta 2 is (5000, 25)\n",
      "Shape of Theta2_grad is (10, 25)\n",
      "Shape of Theta1_grad is (25, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 4.15336892e-09, ...,\n",
       "       5.00631408e-04, 1.13453370e-03, 1.35707295e-03])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checking_funct(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    \n",
    "    X = np.concatenate([np.ones((m,1)), X], axis = 1)\n",
    "    z_2 = X.dot(Theta1.T)\n",
    "    a_2 = utils.sigmoid(z_2)\n",
    "    a_2 = np.concatenate([np.ones((m ,1)), a_2], axis = 1)\n",
    "    h_X = (utils.sigmoid(a_2.dot(Theta2.T)))\n",
    "    \n",
    "    delta_3 = h_X - temp_y\n",
    "    delta_2 = np.delete(delta_3.dot(Theta2), 0, 1)  * sigmoidGradient(z_2)\n",
    "    \n",
    "    Theta2_grad = np.delete(delta_3.T.dot(a_2), 0, 1)/m\n",
    "    Theta1_grad = np.delete(delta_2.T.dot(X), 0, 1)/m\n",
    "    \n",
    "    \n",
    "    print('Shape of delta 2 is', delta_2.shape)\n",
    "    print('Shape of Theta2_grad is', Theta2_grad.shape)\n",
    "    print('Shape of Theta1_grad is', Theta1_grad.shape)\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    return grad\n",
    "    \n",
    "checking_funct(nn_params,\n",
    "               input_layer_size,\n",
    "               hidden_layer_size,\n",
    "               num_labels,\n",
    "               X, y, lambda_=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "759b380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    temp_y = np.zeros((m, num_labels))\n",
    "    for i, num in enumerate(y):\n",
    "        temp_y[i][num] = 1\n",
    "    \n",
    "    \n",
    "    X = np.concatenate([np.ones((m,1)), X], axis = 1)\n",
    "    z_2 = X.dot(Theta1.T)\n",
    "    a_2 = utils.sigmoid(z_2)\n",
    "    a_2 = np.concatenate([np.ones((m ,1)), a_2], axis = 1)\n",
    "    h_X = (utils.sigmoid(a_2.dot(Theta2.T)))\n",
    "#     for i in range (m):\n",
    "#         for k in range (10):\n",
    "#             J = J + (- temp_y[i][k] * np.log(h_X[i][k]) - (1 - temp_y[i][k]) * np.log(1 - h_X[i][k]))\n",
    "#     J = J/m\n",
    "\n",
    "    for i in range (m):\n",
    "        J = J + (-temp_y[i].dot(np.log(h_X[i].T)) - (1 - temp_y[i]).dot(np.log(1- h_X[i].T)))\n",
    "    J  = J/m + lambda_ * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:, 1:]**2)) / (2 * m)\n",
    "    \n",
    "    delta_3 = h_X - temp_y\n",
    "    delta_2 = np.delete(delta_3.dot(Theta2), 0, 1)  * sigmoidGradient(z_2)\n",
    "    \n",
    "    Theta2_grad = (delta_3.T.dot(a_2))/m\n",
    "    Theta1_grad = (delta_2.T.dot(X))/m\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "972515f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.383770 \n",
      "The cost should be about                   : 0.287629.\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0\n",
    "J,_ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_ = 1)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f ' % J)\n",
    "print('The cost should be about                   : 0.287629.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "946f262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-3.04978709e-06 -3.04978914e-06]\n",
      " [-1.75060082e-04 -1.75060082e-04]\n",
      " [-9.62660618e-05 -9.62660620e-05]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 1.42869427e-05  1.42869443e-05]\n",
      " [ 2.33146358e-04  2.33146357e-04]\n",
      " [ 1.17982666e-04  1.17982666e-04]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [-2.59383071e-05 -2.59383100e-05]\n",
      " [-2.87468729e-04 -2.87468729e-04]\n",
      " [-1.37149709e-04 -1.37149706e-04]\n",
      " [ 7.62813551e-03  7.62813551e-03]\n",
      " [ 3.69883213e-05  3.69883234e-05]\n",
      " [ 3.35320347e-04  3.35320347e-04]\n",
      " [ 1.53247077e-04  1.53247082e-04]\n",
      " [-6.74798370e-03 -6.74798370e-03]\n",
      " [-4.68759764e-05 -4.68759769e-05]\n",
      " [-3.76215588e-04 -3.76215587e-04]\n",
      " [-1.66560294e-04 -1.66560294e-04]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.64090819e-01  1.64090819e-01]\n",
      " [ 1.64567932e-01  1.64567932e-01]\n",
      " [ 1.58339334e-01  1.58339334e-01]\n",
      " [ 1.51127527e-01  1.51127527e-01]\n",
      " [ 1.49568335e-01  1.49568335e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 5.75736493e-02  5.75736493e-02]\n",
      " [ 5.77867378e-02  5.77867378e-02]\n",
      " [ 5.59235296e-02  5.59235296e-02]\n",
      " [ 5.36967009e-02  5.36967009e-02]\n",
      " [ 5.31542052e-02  5.31542052e-02]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 5.04575855e-02  5.04575855e-02]\n",
      " [ 5.07530173e-02  5.07530173e-02]\n",
      " [ 4.91620841e-02  4.91620841e-02]\n",
      " [ 4.71456249e-02  4.71456249e-02]\n",
      " [ 4.65597186e-02  4.65597186e-02]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.38295e-11\n"
     ]
    }
   ],
   "source": [
    "utils.checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da67165",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.4 Backpropagation\n",
    "\n",
    "![](Figures/ex4-backpropagation.png)\n",
    "\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis $h_\\theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an “error term” $\\delta_j^{(l)}$ that measures how much that node was “responsible” for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$. In detail, here is the backpropagation algorithm (also depicted in the figure above). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for-loop `for t in range(m)` and place steps 1-4 below inside the for-loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)}, y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Set the input layer’s values $(a^{(1)})$ to the $t^{th }$training example $x^{(t)}$. Perform a feedforward pass, computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a `+1` term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. In `numpy`, if a 1 is a column matrix, adding one corresponds to `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), set \n",
    "$$\\delta_k^{(3)} = \\left(a_k^{(3)} - y_k \\right)$$\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$. You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "1. For the hidden layer $l = 2$, set \n",
    "$$ \\delta^{(2)} = \\left( \\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $*$ performs element wise multiplication in `numpy`.\n",
    "\n",
    "1. Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$. In `numpy`, removing $\\delta_0^{(2)}$ corresponds to `delta_2 = delta_2[1:]`.\n",
    "$$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^{(T)} $$\n",
    "\n",
    "1. Obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Python/Numpy tip**: You should implement the backpropagation algorithm only after you have successfully completed the feedforward and cost functions. While implementing the backpropagation alogrithm, it is often useful to use the `shape` function to print out the shapes of the variables you are working with if you run into dimension mismatch errors.\n",
    "</div>\n",
    "\n",
    "[Click here to go back and update the function `nnCostFunction` with the backpropagation algorithm](#nnCostFunction).\n",
    "\n",
    "\n",
    "**Note:** If the iterative solution provided above is proving to be difficult to implement, try implementing the vectorized approach which is easier to implement in the opinion of the moderators of this course. You can find the tutorial for the vectorized approach [here](https://www.coursera.org/learn/machine-learning/discussions/all/threads/a8Kce_WxEeS16yIACyoj1Q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6b2b75e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  5]\n",
      " [ 4  6  7]\n",
      " [ 3 10  7]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,3,4], [3,5,6], [2,9,6]])\n",
    "b = np.array([[1,2],[2,3],[3,4]])\n",
    "b_ = np.delete(b,0,1)\n",
    "c = a\n",
    "c = np.delete(c, 0, 1)\n",
    "for i in range (3):\n",
    "    for j in range (3):\n",
    "        a[i,j] = a[i,j] + 1\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5b1e8b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-95-4f80e5e8a5ff>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-95-4f80e5e8a5ff>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <a id=\"section5\"></a>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 2.5 Regularized Neural Network\n",
    "\n",
    "After you have successfully implemented the backpropagation algorithm, you will add regularization to the gradient. To account for regularization, it turns out that you can add this as an additional term *after* computing the gradients using backpropagation.\n",
    "\n",
    "Specifically, after you have computed $\\Delta_{ij}^{(l)}$ using backpropagation, you should add regularization using\n",
    "\n",
    "$$ \\begin{align} \n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that you should *not* be regularizing the first column of $\\Theta^{(l)}$ which is used for the bias term. Furthermore, in the parameters $\\Theta_{ij}^{(l)}$, $i$ is indexed starting from 1, and $j$ is indexed starting from 0. Thus, \n",
    "\n",
    "$$\n",
    "\\Theta^{(l)} = \\begin{bmatrix}\n",
    "\\Theta_{1,0}^{(i)} & \\Theta_{1,1}^{(l)} & \\cdots \\\\\n",
    "\\Theta_{2,0}^{(i)} & \\Theta_{2,1}^{(l)} & \\cdots \\\\\n",
    "\\vdots &  ~ & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "[Now modify your code that computes grad in `nnCostFunction` to account for regularization.](#nnCostFunction)\n",
    "\n",
    "After you are done, the following cell runs gradient checking on your implementation. If your code is correct, you should expect to see a relative difference that is less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44561f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
